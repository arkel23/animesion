# Tagging
![](./classification_tagging/data_exploration/figures/AnimesionSystemDiagramRT.png)

We study multi-label classification (also known as tagging) as the problem of learning a mapping 
from a sequence of input query tokens to a set of prediction tags. In this way, it's much more similar to masked language modeling (MLM) or image captioning than the standard way of studying it as a multi-label classification problem. This method is in contrast to the traditional approach of utilizing binary cross-entropy and predict probability of each tag individually.

However, most transformers methods employed in vision-language tasks including MLM require a large-scale pretraining. ViLT, for example, does roughly the equivalent of 1,170 epochs on DAF:re training set, with a larger image size and text input length (and therefore a much larger effective input sequence length), on an effectively much larger dataset.

Therefore, we study the feasibility of this task by utilizing ViLT B-16, with both a character classification head and a MLM head, utilizing image size 128x128, mini-batch size 16, and 16 text tokens as input, on \textit{DAF:re} for 100 epochs, with a cosine LR schedule. The outputs can be WordPiece tokens, or entire tags, depending on the tokenizer used. The sequence of input token queries can be either a fixed token ID, or any random token ID in tokenizer vocabulary (30,522 in case of BERT WordPiece tokenizer or 13,504 in case of tag tokenizing). We compare the input queries (constant or random), and the tokenizer choice (WP or Tag).

Additionally, for this task we propose using a gradual learning schedule inspired by curriculum learning, where the model input queries change as training progresses. The training for this schedule, that we coin as sigmoid masking schedule, is divided into three stages. In the first stage, the model gets the unaltered text tokens as inputs, in order to learn to effectively use multimodal tag data with vision data. In the second stage, the model still keeps a percentage of text tokens as unaltered inputs, but a percentage is modified to either a fixed ID or a random token ID, following a sigmoid function with respect to the training process. In the third stage, the model gets all input tokens as queries, either fixed or random, and therefore all tokens must be predicted. A visualization of the sigmoid masking schedule is shown below:

![](./data_exploration/figures/mask_sigmoid.png)

We compare this method against a fixed, predicting all tokens since the beginning scheme. We compare these results qualitatively, by looking at the output tags from the model.

The input sequence consists of 16 tokens, either random or fixed, but the first and last token are fixed to always be [CLS] and [SEP], regardless of the training schedule, and therefore are not changed, same for [PAD] tokens that are added for images with not enough tags to reach 16 tags in either tokenizer scheme. The outputs displayed in below figure represent the output after training for 100 epochs with the listed training scheme, query token behavior, and tokenizing scheme. We filter the outputs by returning only unique tags, as many times the outputs contain the same tag twice or even more times. In the case of the WP tokenizer, since it returns a string of text, we do a sub-string search of all the tags in the dataset, and return that tag if that sub-string is part of the output. This explains why most WP outputs contain similar outputs such as *black* and *black hair*. We can see that, in general, the tag tokenizing scheme performs much better than the WP tokenizing scheme. Additionally, the fixed schedule where all tokens are changed from the start, outputs more unique tokens, in comparison to the proposed sigmoid schedule. There's not much difference between the outputs of the constant and random query token behavior, but it seems the constant behavior outputs more unique tokens.

![](./data_exploration/figures/TagSample.png)